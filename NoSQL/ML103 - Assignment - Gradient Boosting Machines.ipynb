{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Dive into Boosting Algorithms\n",
    "- Assignment by Pranav Shah & Nayan Tejani\n",
    "- BDAP PT - Batch 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Boosting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Background: -\n",
    "- A common task that appears in different machine learning applications is to build a non-parametric regression or classification model from the data. \n",
    "- When designing a model in domain-specific areas, one strategy is to build a model from theory and adjust its parameters based on the observed data. \n",
    "- Seldom applicable as in most real-life situations such models are not available \n",
    "- also relationships between the input variables are not available to researcher\n",
    "- Hence the need to consider non-prarametric machine learning techniques / algorithms, also known as data-driven modeling\n",
    "\n",
    "Every predictive model provides a specific estimate of the response variable and this accuracy can be gradually boosted in broadly 2 ways \n",
    "    - Feature Engineering\n",
    "    - Applying Boosting Algorithms\n",
    "\n",
    "An ensemble is a collection of predictors whose predictions are combined usually by some sort of weighted average or vote in order to provide an overall prediction that takes its guidance from the collection itself.\n",
    "   \n",
    "Definintion of Boosting: -\n",
    "    Boosting is an ensemble technique for creating a collection of Predictors \n",
    "        - using a method of intelligently selecting random samples of data, \n",
    "        - build learning algorithms and take simple means to find accuracry & estimates \n",
    "        - Learners are trained sequentially & errors of early predictions indicate the \"hard to classify\" observations \n",
    "        - in further Iterations subsequently give more and more weight to convert weak learners into complex predictors\n",
    "        - thereby Focus later predictions on getting these examples right    \n",
    "    \n",
    "Boosting helps to efficiently optimize the estimate of response variable and also gradually increase the accuracy.\n",
    "\n",
    "- The family of boosting methods is based on a different, constructive strategy of ensemble formation. \n",
    "- The main idea of boosting is to add new models to the ensemble sequentially. \n",
    "- At each particular iteration, a new weak, base-learner model is trained with respect to the error of the whole ensemble learnt so far\n",
    "- There are multiple boosting algorithms like Gradient Boosting, XGBoost, AdaBoost, Gentle Boost etc. \n",
    "- Every algorithm has its own underlying mathematics and a slight variation is observed while applying them\n",
    "\n",
    "\n",
    "Boosting algorithms play a crucial role in dealing with bias variance trade-off.  \n",
    "Unlike bagging algorithms, which only controls for high variance in a model, \n",
    "boosting controls both the aspects (bias & variance), and is considered to be more effective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are Gradient Boosting Machines?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Background    \n",
    "- To establish a connection with the statistical framework, a gradient-descent based formulation of boosting methods was derived\n",
    "- This formulation of boosting methods and the corresponding models were called the gradient boosting machines\n",
    "\n",
    "Definition of Gradient Boosting: -\n",
    "Gradient Boosting is a technique for producing regression models consisting of a collection of regressors where-in\n",
    "    - Initial learner is a single layer decision tree predictor\n",
    "    - regression predictor is learnt & error residual is computed\n",
    "    - Error residual is predicted as function and fit back into learners to create a 2-layer decision tree predictor & so on....\n",
    "    - Such that the sum of Predictions is increasingly accurate & Predictive function is increasingly complex\n",
    "    \n",
    "\n",
    "In gradient boosting machines, or simply, GBMs, the learning procedure consecutively fits new models to provide a more accurate estimate of the response variable\n",
    "The principle idea behind this algorithm is to construct the new base-learners to be maximally correlated with the negative gradient of the loss function, associated with the whole ensemble.\n",
    "The loss functions applied can be arbitrary, but to give a better intuition, if the error function is the classic squared-error loss, the learning procedure would result in consecutive error-fitting\n",
    "This high flexibility makes the GBMs highly customizable to any particular data-driven task and also introduces a lot of freedom into the model design\n",
    "\n",
    "GB builds an additive model in a forward stage-wise fashion; allows for the optimization of arbitrary differentiable loss functions.\n",
    "In each stage a regression tree is fit on the negative gradient of the given loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are the (hyper)parameters of the GBM algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sklearn.ensemble.GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, \n",
    "                                                  min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n",
    "                                                  max_depth=3, init=None, random_state=None, max_features=None, verbose=0,\n",
    "                                                  max_leaf_nodes=None, warm_start=False, presort='auto')\n",
    "\n",
    "The overall parameters can be divided into 3 categories:\n",
    "\n",
    "1. Tree-Specific Parameters: These affect each individual tree in the model.\n",
    "2. Boosting Parameters: These affect the boosting operation in the model.\n",
    "3. Miscellaneous Parameters: Other parameters for overall functioning\n",
    "    \n",
    "Below are the parameters: -\n",
    "# Tree-Specific\n",
    "[1.A]    min_samples_split\n",
    "            Defines the minimum number of samples (or observations) required in a node to be considered for splitting.\n",
    "            Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "            Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "[1.B]    min_samples_leaf\n",
    "            Defines the minimum samples (or observations) required in a terminal node or leaf.\n",
    "            Used to control over-fitting similar to min_samples_split.\n",
    "            Generally lower values should be chosen for imbalanced class problems because the regions in which the minority class will be in majority will be very small.\n",
    "[1.C]    min_weight_fraction_leaf\n",
    "            Similar to min_samples_leaf but defined as a fraction of the total number of observations instead of an integer.\n",
    "            Only one of #2 and #3 should be defined.\n",
    "[1.D]    max_depth\n",
    "            The maximum depth of a tree.\n",
    "            Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
    "            Should be tuned using CV.\n",
    "[1.E]    max_leaf_nodes\n",
    "            The maximum number of terminal nodes or leaves in a tree.\n",
    "            Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "            If this is defined, GBM will ignore max_depth.\n",
    "[1.F]    max_features\n",
    "            The number of features to consider while searching for a best split. These will be randomly selected.\n",
    "            As a thumb-rule, square root of the total number of features works great but we should check upto 30-40% of the total number of features.\n",
    "            Higher values can lead to over-fitting but depends on case to case.\n",
    "# Boosting\n",
    "[2.A]    learning_rate\n",
    "            This determines the impact of each tree on the final outcome. GBM works by starting with an initial estimate which is updated using the output of each tree. The learning parameter controls the magnitude of this change in the estimates.\n",
    "            Lower values are generally preferred as they make the model robust to the specific characteristics of tree and thus allowing it to generalize well.\n",
    "            Lower values would require higher number of trees to model all the relations and will be computationally expensive.\n",
    "[2.B]    n_estimators\n",
    "            The number of sequential trees to be modeled (step 2)\n",
    "            Though GBM is fairly robust at higher number of trees but it can still overfit at a point. Hence, this should be tuned using CV for a particular learning rate.\n",
    "[2.C]    subsample\n",
    "            The fraction of observations to be selected for each tree. Selection is done by random sampling.\n",
    "            Values slightly less than 1 make the model robust by reducing the variance.\n",
    "            Typical values ~0.8 generally work fine but can be fine-tuned further\n",
    "            \n",
    "# Miscellaneous\n",
    "[3.A]    loss\n",
    "            It refers to the loss function to be minimized in each split.\n",
    "            It can have various values for classification and regression case. Generally the default values work fine. \n",
    "            Other values should be chosen only if you understand their impact on the model.\n",
    "[3.B]    init\n",
    "            This affects initialization of the output.\n",
    "            This can be used if we have made another model whose outcome is to be used as the initial estimates for GBM.\n",
    "[3.C]    random_state\n",
    "            The random number seed so that same random numbers are generated every time.\n",
    "            This is important for parameter tuning. If we don’t fix the random number, then we’ll have different outcomes for subsequent runs on the same parameters and it becomes difficult to compare models.\n",
    "            It can potentially result in overfitting to a particular random sample selected. We can try running models for different random samples, which is computationally expensive and generally not used.\n",
    "[3.D]    verbose\n",
    "            The type of output to be printed when the model fits. The different values can be:\n",
    "            0: no output generated (default)\n",
    "            1: output generated for trees in certain intervals\n",
    "            >1: output generated for all trees\n",
    "[3.E]    warm_start\n",
    "            This parameter has an interesting application and can help a lot if used judicially.\n",
    "            Using this, we can fit additional trees on previous fits of a model. It can save a lot of time and you should explore this option for advanced applications\n",
    "[3.F]    presort \n",
    "            Select whether to presort data for faster splits.\n",
    "            It makes the selection automatically by default but it can be changed if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost is short for “Extreme Gradient Boosting”, a project for supervised learning using Greedy Gradient Boosting Algorithms\n",
    "It is an R package for Fast and Accurate Gradient Boosting\n",
    "\n",
    "- An open-sourced tool with Computation in C++ and provides Interfaces for R/Python/Julia\n",
    "- A variant of the gradient boosting machine leveraging Tree-based model\n",
    "- XGBoost is a multi-language library designed and optimized for boosting trees algorithms. \n",
    "- The underlying algorithm of xgboost is an extension of the classic gradient boosting machine algorithm. \n",
    "- By employing multi-threads and imposing regularization, xgboost is able to utilize more computational power and get more accurate prediction compared to the traditional version. \n",
    "- Moreover, a friendly user interface and comprehensive documentation are provided for user convenience.\n",
    "- widely applied in both industrial business and academic researches\n",
    "\n",
    "\n",
    "The goal of this library is to push the extreme of the computation limits of machines to provide a scalable, portable and accurate library\n",
    "It is developed with both deep consideration in terms of systems optimization and principles in machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An example of xgboost in practice (small end-to-end code sample without any tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Install Xgboost\n",
    "sudo pip install xgboost\n",
    "sudo pip install --upgrade xgboost\n",
    "\n",
    "import numpy\n",
    "import xgboost\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import accuracy_score\n",
    "### Example of XGBoost\n",
    "### Assuming XGBoost is installed on the machine\n",
    "### Set working directory to the folder where csv is saved\n",
    "\n",
    "# load data\n",
    "diabet.dataset = numpy.loadtxt('pima-indians-diabetes.csv', delimiter=\",\")\n",
    "# split data into X and y\n",
    "indep.dataset = dataset[:,0:8]\n",
    "depen.dataset = dataset[:,8]\n",
    "# split data into train and test sets\n",
    "seed = 7\n",
    "test.size = 0.3\n",
    "indep.train, indep.test, depen.train, depen.test = cross_validation.train_test_split(indep.dataset, depen.dataset, test.size=test.size, random_state=seed)\n",
    "# fit model no training data\n",
    "model = xgboost.XGBClassifier()\n",
    "model.fit(indep.train, depen.train)\n",
    "# make predictions for test data\n",
    "test.pred = model.predict(indep.test)\n",
    "predictions = [round(value) for value in test.pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(test.pred, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "##### Accuracy comes close to 78%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does one tune the hyperparameters of GBM/xgboost in practice?\n",
    "- What are good initial values to start with?\n",
    "- Any rules of thumb?\n",
    "- Grid search and Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "There are two types of parameter to be tuned here – tree based and boosting parameters. \n",
    "There are no optimum values for learning rate as low values always work better, given that we train on sufficient number of trees\n",
    "A high number for pa particular learning rate can lead to overfitting. \n",
    "But as we reduce the learning rate and increase trees, the computation becomes expensive and would take a long time to run on standard personal computers\n",
    "\n",
    "The following approach may be taken for tuning the hyperparameters of GBM/xgboost in practice: -\n",
    "\n",
    "1. Choose a relatively high learning rate. \n",
    "    Generally the default value of 0.1 works but somewhere between 0.05 to 0.2 should work for different problems\n",
    "2. Determine the optimum number of trees for this learning rate. \n",
    "    This should range around 40-70. Remember to choose a value on which your system can work fairly fast. This is because it will be used for testing various scenarios and determining the tree parameters.\n",
    "3. Tune tree-specific parameters for decided learning rate and number of trees. \n",
    "    Note that we can choose different parameters to define a tree and I’ll take up an example here.\n",
    "4. Lower the learning rate and increase the estimators proportionally to get more robust models.\n",
    "\n",
    "\n",
    "### Good initial Values to start with\n",
    "\n",
    "Fix learning rate and number of estimators for tuning tree-based parameters\n",
    "\n",
    "In order to decide on boosting parameters, we need to set some initial values of other parameters. \n",
    "Lets take the following values:\n",
    "\n",
    "min_samples_split = 500 : This should be ~0.5-1% of total values. Since this is imbalanced class problem, we’ll take a small value from the range.\n",
    "min_samples_leaf = 50 : Can be selected based on intuition. This is just used for preventing overfitting and again a small value because of imbalanced classes.\n",
    "max_depth = 8 : Should be chosen (5-8) based on the number of observations and predictors. This has 87K rows and 49 columns so lets take 8 here.\n",
    "max_features = ‘sqrt’ : Its a general thumb-rule to start with square root.\n",
    "subsample = 0.8 : This is a commonly used used start value\n",
    "    \n",
    "### Any rules of thumb\n",
    "Few rules to consider when tuning the hyperparameters of GBM / xgboost: -\n",
    "    1. Drop a categorical variable in case of too many categories\n",
    "    2. Identify conversion / transformation variables if any and use the transformed variable - drop out the originial variable\n",
    "        (for e.g. Date of birth could be converted into Age, use age and drop Date of birth)\n",
    "    3. Identify the Variables having missing value in too many records - create Missing Variable column and use 1/0 in case if missing / present. Drop the original columns\n",
    "    4. Impute a particular values with 0(as median) in case if value missing for a minimal % of total records. \n",
    "        For e.g. if 2% of total record set have missing value for a column, cannot drop the entire column. Instead use 0 in case of missing value for those records\n",
    "    5. Identify variables which make lesser / little intuitive impact on outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An example of hyperparameter tuning with xgboost (code) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.grid_search import GridSearchCV, RamdomizedSearchCV\n",
    "from sklearn.datasets import make_classifications\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "from scipy.stats import randint,uniform\n",
    "##reproductivity\n",
    "import xgboost \n",
    "seed=342\n",
    "np.random.seed(seed)\n",
    "\n",
    "x, y = make_classification (n_samples=1000,n_features=20, n_informative=8, n_redundant=3, n_repeated=2,random_state=seed)\n",
    "\n",
    "### Cross Validations \n",
    "cv = StratifiedKFold(y, n_folds=10, shuffle =True, random_state=seed)\n",
    "\n",
    "#### Grid Search HyperTuning\n",
    "params_grid = {\n",
    "               'max_depth':[1,2,3],\n",
    "               'n_estimators' : [5,10,25,20],\n",
    "               'learning_rate' : np.linespace[1e-16, 1,3]\n",
    "              }\n",
    "\n",
    "### Create disctionary\n",
    "params_fixed = {\n",
    "                'objective' : 'binary:logistic',\n",
    "                'silent' :  1\n",
    "}\n",
    "\n",
    "#########Grid Search CV\n",
    "bst_grid = GridSearchCV(\n",
    "              estimator=XGBClassifier(params_fixed, seed=seed),\n",
    "              param_grid=params_grid,\n",
    "              cv=cv,\n",
    "              scoring ='accuarcy'\n",
    ")\n",
    "\n",
    "bst_grid.fit(x, y)\n",
    "bst_grid.grid_scores_\n",
    "\n",
    "print(\"Best accuracy obtained: {0}\".format(bst_grid.best_score_))\n",
    "print(\"Parameters:\")\n",
    "\n",
    "for key, value in best_grid.best_params_.items():\n",
    "    print(\"\\t{}: {}\".format(key,value))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}